{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief inplementatin of the LangGraph\n",
    "\n",
    "Our solution incorporates a complex LangGraph orchestrated flow, keeping the User interactively in the Loop.\n",
    "\n",
    "The user provides input, which is then processed by the **Retriever** and checked against a **Prompt Verificator** to determine whether the context is sufficient and relevant. If not, the user is guided to cooperate until a good similarity threshold is met.\n",
    "\n",
    "If the context is sufficient, the **Writer** drafts a story and sends it to the **Watcher**, who is responsible for monitoring the process, keeping memory of iterations, and interacting with the user.\n",
    "\n",
    "The Draft is sent to the **Editor**, who checks against storytelling protocols and searches for grammar/lexical mistakes. Multiple loops are executed until the protocols are met.\n",
    "\n",
    "During contextual and character embedding, different LLM models are used depending on their limit and inference speed. Our Final Editor utilizes a fine-tuned Ollama-serviced BGgpt model aligned with best editor practices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **User Interaction**:\n",
    "   - The user provides input, such as a request for a story about a specific character or theme.\n",
    "   - The system may ask for additional personalized details like the main character, environment, or moral/lesson.\n",
    "\n",
    "2. **Retriever**:\n",
    "   - The input is processed by the Retriever, which uses a Vector DB with contextual and character LLM-enhanced embeddings.\n",
    "   - It checks if the context is enough and relevant.\n",
    "\n",
    "3. **Prompt Verificator**:\n",
    "   - If the context is not sufficient, the Prompt Verificator checks the similarity threshold and may prompt the user for more information.\n",
    "\n",
    "4. **Writer**:\n",
    "   - If the context is sufficient, the Writer drafts a story based on the input.\n",
    "\n",
    "5. **Watcher**:\n",
    "   - The Watcher monitors the process, updating the state and allowing for a maximum of three drafts.\n",
    "   - It interacts with the user if necessary.\n",
    "\n",
    "6. **Editor**:\n",
    "   - The draft is checked by the Editor against storytelling protocols and for grammar/lexical mistakes using a BGgpt model serviced by Ollama.\n",
    "\n",
    "7. **Success**:\n",
    "   - If the draft passes the checks, the process is marked as successful.\n",
    "\n",
    "This flowchart outlines a structured approach to generating personalized stories, ensuring quality and relevance through multiple stages of verification and editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq langchain-groq==0.2.0 --progress-bar off\n",
    "!pip install -qqq langgraph==0.2.22 --progress-bar off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import textwrap\n",
    "from enum import Enum, auto\n",
    "from typing import List, Literal, Optional, TypedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import userdata\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import END, StateGraph\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "MODEL = \"llama-3.1-70b-versatile\"\n",
    "\n",
    "llm = ChatGroq(temperature=0, model_name=MODEL, api_key=userdata.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "EDITOR_PROMPT = \"\"\"\n",
    "Rewrite for maximum social media engagement:\n",
    "\n",
    "- Use attention-grabbing, concise language\n",
    "- Inject personality and humor\n",
    "- Optimize formatting (short paragraphs)\n",
    "- Encourage interaction (questions, calls-to-action)\n",
    "- Ensure perfect grammar and spelling\n",
    "- Rewrite from first person perspective, when talking to an audience\n",
    "\n",
    "Use only the information provided in the text. Think carefully.\n",
    "\"\"\"\n",
    "\n",
    "TWITTER_PROMPT = \"\"\"\n",
    "Generate a high-engagement tweet from the given text:\n",
    "1. What problem does this solve?\n",
    "2. Focus on the main technical points/features\n",
    "3. Write a short, coherent paragraph (2-3 sentences max)\n",
    "4. Use natural, conversational language\n",
    "5. Optimize for virality: make it intriguing, relatable, or controversial\n",
    "6. Exclude emojis and hashtags\n",
    "\"\"\"\n",
    "\n",
    "TWITTER_CRITIQUE_PROMPT = \"\"\"\n",
    "You are a Tweet Critique Agent. Your task is to analyze tweets and provide actionable feedback to make them more engaging. Focus on:\n",
    "\n",
    "1. Clarity: Is the message clear and easy to understand?\n",
    "2. Hook: Does it grab attention in the first few words?\n",
    "3. Brevity: Is it concise while maintaining impact?\n",
    "4. Call-to-action: Does it encourage interaction or sharing?\n",
    "5. Tone: Is it appropriate for the intended audience?\n",
    "6. Storytelling: Does it evoke curiosity?\n",
    "7. Remove hype: Does it promise more than it delivers?\n",
    "\n",
    "Provide 2-3 specific suggestions to improve the tweet's engagement potential.\n",
    "Do not suggest hashtags. Keep your feedback concise and actionable.\n",
    "\n",
    "Your goal is to help the writer improve their social media writing skills and increase engagement with their posts.\n",
    "\"\"\"\n",
    "\n",
    "# Graph\n",
    "\n",
    "class Post(BaseModel):\n",
    "    \"\"\"A post written in different versions\"\"\"\n",
    "\n",
    "    drafts: List[str]\n",
    "    feedback: Optional[str]\n",
    "\n",
    "\n",
    "class AppState(TypedDict):\n",
    "    user_text: str\n",
    "    target_audience: str\n",
    "    edit_text: str\n",
    "    tweet: Post\n",
    "    linkedin_post: Post\n",
    "    n_drafts: int\n",
    "\n",
    "# Nodes\n",
    "\n",
    "# Editor\n",
    "def editor_node(state: AppState):\n",
    "    prompt = f\"\"\"\n",
    "text:\n",
    "```\n",
    "{state[\"user_text\"]}\n",
    "```\n",
    "\"\"\".strip()\n",
    "    response = llm.invoke([SystemMessage(EDITOR_PROMPT), HumanMessage(prompt)])\n",
    "    return {\"edit_text\": response.content}\n",
    "\n",
    "\n",
    "# Tweet Writer\n",
    "def tweet_writer_node(state: AppState):\n",
    "\n",
    "    post = state[\"tweet\"]\n",
    "\n",
    "    feedback_prompt = (\n",
    "        \"\"\n",
    "        if not post.feedback\n",
    "        else f\"\"\"\n",
    "Tweet:\n",
    "```\n",
    "{post.drafts[-1]}\n",
    "```\n",
    "\n",
    "Use the feedback to improve it:\n",
    "```\n",
    "{post.feedback}\n",
    "```\n",
    "\"\"\".strip()\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "text:\n",
    "```\n",
    "{state[\"edit_text\"]}\n",
    "```\n",
    "\n",
    "{feedback_prompt}\n",
    "\n",
    "Target audience: {state[\"target_audience\"]}\n",
    "\n",
    "Write only the text for the post\n",
    "\"\"\".strip()\n",
    "\n",
    "    response = llm.invoke([SystemMessage(TWITTER_PROMPT), HumanMessage(prompt)])\n",
    "    post.drafts.append(response.content)\n",
    "    return {\"tweet\": post}\n",
    "\n",
    "# Tweet Critique\n",
    "def critique_tweet_node(state: AppState):\n",
    "    post = state[\"tweet\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Full post:\n",
    "```\n",
    "{state[\"edit_text\"]}\n",
    "```\n",
    "\n",
    "Suggested tweet (critique this):\n",
    "```\n",
    "{post.drafts[-1]}\n",
    "```\n",
    "\n",
    "Target audience: {state[\"target_audience\"]}\n",
    "\"\"\".strip()\n",
    "\n",
    "    response = llm.invoke(\n",
    "        [SystemMessage(TWITTER_CRITIQUE_PROMPT), HumanMessage(prompt)]\n",
    "    )\n",
    "    post.feedback = response.content\n",
    "    return {\"tweet\": post}\n",
    "\n",
    "\n",
    "def supervisor_node(state: AppState):\n",
    "    return state\n",
    "\n",
    "# Edges\n",
    "def should_rewrite(\n",
    "    state: AppState,\n",
    ") -> Literal[[\"tweet_critique\"], END]:\n",
    "    tweet = state[\"tweet\"]\n",
    "    if len(tweet.drafts) >= n_drafts:\n",
    "        return END\n",
    "\n",
    "    return [\"tweet_critique\"]\n",
    "\n",
    "graph = StateGraph(AppState)\n",
    "\n",
    "graph.add_node(\"editor\", editor_node)\n",
    "graph.add_node(\"tweet_writer\", tweet_writer_node)\n",
    "graph.add_node(\"tweet_critique\", critique_tweet_node)\n",
    "graph.add_node(\"supervisor\", supervisor_node)\n",
    "\n",
    "graph.add_edge(\"editor\", \"tweet_writer\")\n",
    "\n",
    "graph.add_edge(\"tweet_writer\", \"supervisor\")\n",
    "graph.add_conditional_edges(\"supervisor\", should_rewrite)\n",
    "\n",
    "graph.add_edge(\"tweet_critique\", \"tweet_writer\")\n",
    "\n",
    "graph.set_entry_point(\"editor\")\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))\n",
    "\n",
    "%%time\n",
    "config = {\"configurable\": {\"thread_id\": 42}}\n",
    "\n",
    "user_text = \"\"\"\n",
    "With 22 billion parameters, Mistral Small v24.09 offers customers a convenient mid-point between Mistral NeMo 12B and Mistral Large 2,\n",
    "providing a cost-effective solution that can be deployed across various platforms and environments.\n",
    "The new small model delivers significant improvements in human alignment, reasoning capabilities, and code over the previous model.\n",
    "\n",
    "Mistral-Small-Instruct-2409 is an instruct fine-tuned version with the following characteristics:\n",
    "\n",
    "- 22B parameters\n",
    "- Vocabulary to 32768\n",
    "- Supports function calling\n",
    "- 128k sequence length\n",
    "\n",
    "Mistral Small v24.09 is released under the MRL license. You may self-deploy it for non-commercial purposes, using e.g. vLLM\n",
    "\n",
    "Weights on HuggingFace hub: https://huggingface.co/mistralai/Mistral-Small-Instruct-2409\n",
    "\"\"\"\n",
    "\n",
    "state = app.invoke(\n",
    "    {\n",
    "        \"user_text\": user_text,\n",
    "        \"target_audience\": \"AI/ML engineers and researchers, Data Scientists\",\n",
    "        \"tweet\": Post(drafts=[], feedback=None),\n",
    "        \"linkedin_post\": Post(drafts=[], feedback=None),\n",
    "        \"n_drafts\": 3,\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(state[\"tweet\"].feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state[\"edit_text\"])\n",
    "\n",
    "for i, draft in enumerate(state[\"tweet\"].drafts):\n",
    "    print(f\"Draft #{i+1}\")\n",
    "    print(\"-\" * 10)\n",
    "    print(textwrap.fill(draft, 80))\n",
    "    print()\n",
    "print(state[\"edit_text\"])\n",
    "for i, draft in enumerate(state[\"tweet\"].drafts):\n",
    "    print(f\"Draft #{i+1}\")\n",
    "    print(\"-\" * 10)\n",
    "    print(textwrap.fill(draft, 80))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\a1bg532573\\AppData\\Local\\Temp\\ipykernel_21020\\935491472.py:18: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fairy tale generation process...\n",
      "\n",
      "--- Writer Node (Draft #1) ---\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from typing import List, Literal, Optional, TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load configuration\n",
    "with open('../config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    os.environ['OPENAI_API_KEY'] = config['OPENAI_API_KEY']\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Set up ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4-0125-preview\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.7,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Prompts\n",
    "WRITER_PROMPT = \"\"\"\n",
    "Generate a fairy tale based on the given input and personalized details:\n",
    "- Incorporate the main character, environment, and moral lesson\n",
    "- Use vivid, engaging language suitable for the target audience\n",
    "- Ensure the story has a clear beginning, middle, and end\n",
    "- Include elements of wonder and magic typical in fairy tales\n",
    "- Aim for a length of about 300 words\n",
    "\"\"\"\n",
    "\n",
    "EDITOR_PROMPT = \"\"\"\n",
    "Review and improve the fairy tale:\n",
    "- Check for consistency with the personalized details\n",
    "- Ensure the story follows the fairy tale structure\n",
    "- Improve language and pacing\n",
    "- Enhance character development and world-building\n",
    "- Strengthen the moral lesson\n",
    "\"\"\"\n",
    "\n",
    "WATCHER_PROMPT = \"\"\"\n",
    "Analyze the fairy tale and provide feedback:\n",
    "1. Engagement: Is the story captivating from the beginning?\n",
    "2. Character development: Are the characters well-defined and relatable?\n",
    "3. Plot: Is the story coherent and well-paced?\n",
    "4. Setting: Is the fairy tale world vividly described?\n",
    "5. Moral lesson: Is the intended lesson clear and impactful?\n",
    "6. Language: Is the writing style appropriate for the target audience?\n",
    "\n",
    "Provide 2-3 specific suggestions to improve the story.\n",
    "Keep your feedback concise and actionable.\n",
    "\"\"\"\n",
    "\n",
    "# Define state and models\n",
    "class FairyTale(BaseModel):\n",
    "    \"\"\"A fairy tale written in different versions\"\"\"\n",
    "    drafts: List[str]\n",
    "    feedback: Optional[str]\n",
    "\n",
    "class AppState(TypedDict):\n",
    "    user_input: str\n",
    "    personalized_details: dict\n",
    "    fairy_tale: FairyTale\n",
    "    n_drafts: int\n",
    "    max_drafts: int\n",
    "\n",
    "# Node functions\n",
    "def writer_node(state: AppState):\n",
    "    print(f\"\\n--- Writer Node (Draft #{state['n_drafts'] + 1}) ---\")\n",
    "    post = state[\"fairy_tale\"]\n",
    "    \n",
    "    feedback_prompt = (\n",
    "        \"\"\n",
    "        if not post.feedback\n",
    "        else f\"Previous feedback:\\n{post.feedback}\\n\\nUse this feedback to improve the story.\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "User input: {state[\"user_input\"]}\n",
    "Personalized details: {state[\"personalized_details\"]}\n",
    "\n",
    "{feedback_prompt}\n",
    "\n",
    "Write a fairy tale based on this information.\n",
    "\"\"\"\n",
    "    response = llm.invoke([SystemMessage(WRITER_PROMPT), HumanMessage(prompt)])\n",
    "    post.drafts.append(response.content)\n",
    "    print(\"New draft created:\")\n",
    "    print(textwrap.fill(response.content, width=80))\n",
    "    return {\"fairy_tale\": post}\n",
    "\n",
    "def editor_node(state: AppState):\n",
    "    print(f\"\\n--- Editor Node (Draft #{state['n_drafts'] + 1}) ---\")\n",
    "    post = state[\"fairy_tale\"]\n",
    "    prompt = f\"\"\"\n",
    "Fairy tale draft:\n",
    "{post.drafts[-1]}\n",
    "\n",
    "Personalized details: {state[\"personalized_details\"]}\n",
    "\n",
    "Review and improve the fairy tale.\n",
    "\"\"\"\n",
    "    response = llm.invoke([SystemMessage(EDITOR_PROMPT), HumanMessage(prompt)])\n",
    "    post.drafts[-1] = response.content\n",
    "    print(\"Edited draft:\")\n",
    "    print(textwrap.fill(response.content, width=80))\n",
    "    return {\"fairy_tale\": post}\n",
    "\n",
    "def watcher_node(state: AppState):\n",
    "    print(f\"\\n--- Watcher Node (Draft #{state['n_drafts'] + 1}) ---\")\n",
    "    post = state[\"fairy_tale\"]\n",
    "    prompt = f\"\"\"\n",
    "Fairy tale:\n",
    "{post.drafts[-1]}\n",
    "\n",
    "Personalized details: {state[\"personalized_details\"]}\n",
    "\n",
    "Analyze and provide feedback on this fairy tale.\n",
    "\"\"\"\n",
    "    response = llm.invoke([SystemMessage(WATCHER_PROMPT), HumanMessage(prompt)])\n",
    "    post.feedback = response.content\n",
    "    print(\"Feedback:\")\n",
    "    print(textwrap.fill(response.content, width=80))\n",
    "    return {\"fairy_tale\": post}\n",
    "\n",
    "# Edges\n",
    "def should_rewrite(state: AppState) -> Literal[\"writer\", END]:\n",
    "    state[\"n_drafts\"] += 1\n",
    "    print(f\"\\nDraft #{state['n_drafts']} completed.\")\n",
    "    if state[\"n_drafts\"] >= state[\"max_drafts\"]:\n",
    "        print(\"Maximum number of drafts reached. Ending process.\")\n",
    "        return END\n",
    "    print(f\"Continuing to Draft #{state['n_drafts'] + 1}.\")\n",
    "    return \"writer\"\n",
    "\n",
    "# Build the graph\n",
    "graph = StateGraph(AppState)\n",
    "\n",
    "graph.add_node(\"writer\", writer_node)\n",
    "graph.add_node(\"editor\", editor_node)\n",
    "graph.add_node(\"watcher\", watcher_node)\n",
    "\n",
    "graph.set_entry_point(\"writer\")\n",
    "graph.add_edge(\"writer\", \"editor\")\n",
    "graph.add_edge(\"editor\", \"watcher\")\n",
    "graph.add_conditional_edges(\"watcher\", should_rewrite)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "# Run the fairy tale generator\n",
    "config = {\"configurable\": {\"thread_id\": 42}}\n",
    "\n",
    "user_input = \"\"\"I want a story for the rabbit Mitko and the cat Yasen. Setting: Magical forest. Moral lesson: The Good always wins.\"\"\"\n",
    "\n",
    "personalized_details = {\n",
    "    \"main_characters\": [\"Mitko the rabbit\", \"Yasen the cat\"],\n",
    "    \"setting\": \"Magical forest\",\n",
    "    \"moral_lesson\": \"The Good always wins\"\n",
    "}\n",
    "\n",
    "initial_state = {\n",
    "    \"user_input\": user_input,\n",
    "    \"personalized_details\": personalized_details,\n",
    "    \"fairy_tale\": FairyTale(drafts=[], feedback=None),\n",
    "    \"n_drafts\": 0,\n",
    "    \"max_drafts\": 3\n",
    "}\n",
    "\n",
    "print(\"Starting fairy tale generation process...\")\n",
    "state = app.invoke(initial_state, config=config)\n",
    "\n",
    "print(\"\\nFinal Fairy Tale:\")\n",
    "print(textwrap.fill(state[\"fairy_tale\"].drafts[-1], width=80))\n",
    "\n",
    "print(f\"\\nTotal number of drafts: {state['n_drafts']}\")\n",
    "\n",
    "print(\"\\nGeneration process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution incorporates a complex LangGraph orchestrated flow, keeping the User interactively in the Loop.\n",
    "\n",
    "The user provides input, which is then processed by the **Retriever** and checked against a **Prompt Verificator** to determine whether the context is sufficient and relevant. If not, the user is guided to cooperate until a good similarity threshold is met.\n",
    "\n",
    "If the context is sufficient, the **Writer** drafts a story and sends it to the **Watcher**, who is responsible for monitoring the process, keeping memory of iterations, and interacting with the user.\n",
    "\n",
    "The Draft is sent to the **Editor**, who checks against storytelling protocols and searches for grammar/lexical mistakes. Multiple loops are executed until the protocols are met.\n",
    "\n",
    "During contextual and character embedding, different LLM models are used depending on their limit and inference speed. Our Final Editor utilizes a fine-tuned Ollama-serviced BGgpt model aligned with best editor practices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairy_tale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
