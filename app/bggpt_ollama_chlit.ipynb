{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "import os\n",
    "import asyncio\n",
    "import threading\n",
    "import requests\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Set up Ollama (you may need to run this in a separate cell)\n",
    "!sudo apt-get install -y pciutils lshw\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Set environment variables for CUDA\n",
    "os.environ['PATH'] += ':/usr/local/cuda/bin'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'\n",
    "# Set the environment variable\n",
    "\n",
    "# Load configuration\n",
    "with open('../config.json') as config_file:\n",
    "    config = json.load(config_file)\n",
    "    os.environ['OPENAI_API_KEY'] = config['OPENAI_API_KEY']\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Ollama setup functions\n",
    "async def run_process(cmd):\n",
    "    print('>>> starting', *cmd)\n",
    "    process = await asyncio.create_subprocess_exec(\n",
    "        *cmd,\n",
    "        stdout=asyncio.subprocess.PIPE,\n",
    "        stderr=asyncio.subprocess.PIPE\n",
    "    )\n",
    "    async def pipe(lines):\n",
    "        async for line in lines:\n",
    "            print(line.decode().strip())\n",
    "    await asyncio.gather(pipe(process.stdout), pipe(process.stderr))\n",
    "\n",
    "async def start_ollama_serve():\n",
    "    await run_process(['ollama', 'serve'])\n",
    "\n",
    "def run_async_in_thread(loop, coro):\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_until_complete(coro)\n",
    "    loop.close()\n",
    "\n",
    "# Start Ollama serve\n",
    "new_loop = asyncio.new_event_loop()\n",
    "thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n",
    "thread.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download bggpt model\n",
    "!ollama pull todorov/bggpt:latest --name False\n",
    "\n",
    "# Constants and configurations\n",
    "CHROMA_PATH = \"full_books_800\"\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\n",
    "Answer always in full sentences.\n",
    "Answer always in Bulgarian.\n",
    "Reformat the result to be more readable.\n",
    "Reformat to exclude repetitions of words.\n",
    "Приказката трябва да бъде дълга и интересна.\n",
    "Приказката трябва да има поучаващ край.\n",
    "Приказката не трябва да бъде със съдържание, което не е подходящо за деца.\n",
    "Приказката трябва да бъде със съдържание, което е подходящо за деца.\n",
    "Приказката не трябва да съдаържа повторения, като един ден, итн.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize embedding function and database\n",
    "embedding_function = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "# Function to generate text using Ollama API\n",
    "def generate_text(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"model\": \"todorov/bggpt:latest\",\n",
    "        \"stream\": False,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.text)[\"response\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response.status_code} {response.text}\")\n",
    "\n",
    "def main():\n",
    "    print(\"Моля, въведете тема за приказката и натиснете Enter.\")\n",
    "\n",
    "    while True:\n",
    "        query_text = input(\"Тема: \").strip()\n",
    "\n",
    "        if not query_text:\n",
    "            print(\"Моля, въведете тема преди да продължите.\")\n",
    "            continue\n",
    "\n",
    "        results = db.similarity_search_with_relevance_scores(query_text, k=1)\n",
    "\n",
    "        if not results:\n",
    "            print(\"Не бяха намерени подходящи резултати. Опитайте с друга тема.\")\n",
    "            continue\n",
    "\n",
    "        context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "        prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "        prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "        response_text = generate_text(prompt)\n",
    "\n",
    "        sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "        sources_text = \", \".join(filter(None, sources))\n",
    "\n",
    "        formatted_response = f\"{response_text}\\n\\nИзточници: {sources_text}\"\n",
    "\n",
    "        print(\"\\n\" + formatted_response + \"\\n\")\n",
    "\n",
    "        if input(\"Искате ли да генерирате друга приказка? (да/не): \").lower() != 'да':\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainlit as cl\n",
    "from pyngrok import ngrok\n",
    "import subprocess\n",
    "import threading\n",
    "import IPython\n",
    "import signal\n",
    "import os\n",
    "import sys\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "public_url = None\n",
    "thread = None\n",
    "process = None\n",
    "stop_flag = threading.Event()\n",
    "\n",
    "# Add these global variables\n",
    "output_buffer = StringIO()\n",
    "error_buffer = StringIO()\n",
    "\n",
    "def run_app():\n",
    "    global process, output_buffer, error_buffer, stop_flag\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            [\"chainlit\", \"run\", \"/content/drive/MyDrive/repos/GenPrikazki/fairy_tale_generator.py\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "\n",
    "        if process is None:\n",
    "            print(\"Failed to start the Chainlit process. Check if Chainlit is installed and the file path is correct.\")\n",
    "            return\n",
    "\n",
    "        while not stop_flag.is_set():\n",
    "            output = process.stdout.readline()\n",
    "            error = process.stderr.readline()\n",
    "\n",
    "            if output:\n",
    "                print(output.strip())\n",
    "                output_buffer.write(output)\n",
    "            if error:\n",
    "                print(f\"Error: {error.strip()}\", file=sys.stderr)\n",
    "                error_buffer.write(error)\n",
    "\n",
    "            if process.poll() is not None:\n",
    "                print(\"Chainlit process has ended unexpectedly.\")\n",
    "                break\n",
    "\n",
    "            time.sleep(0.1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in run_app: {e}\")\n",
    "        print(f\"Process state: {process}\")\n",
    "        if process:\n",
    "            print(f\"Process return code: {process.returncode}\")\n",
    "    finally:\n",
    "        if process:\n",
    "            process.terminate()\n",
    "            process.wait()\n",
    "\n",
    "def print_logs():\n",
    "    global output_buffer, error_buffer\n",
    "    print(\"=== Chainlit App Output ===\")\n",
    "    print(output_buffer.getvalue())\n",
    "    print(\"\\n=== Chainlit App Errors ===\")\n",
    "    print(error_buffer.getvalue())\n",
    "\n",
    "def start_app():\n",
    "    global public_url, thread, stop_flag\n",
    "    stop_flag.clear()\n",
    "\n",
    "    try:\n",
    "        # Try to set up ngrok\n",
    "        try:\n",
    "            public_url = ngrok.connect(8000)\n",
    "            print(f\"Public URL: {public_url}\")\n",
    "        except exception.PyngrokNgrokError as e:\n",
    "            print(f\"Ngrok error: {e}\")\n",
    "            print(\"Continuing without ngrok. You can access the app using Colab's public URL feature.\")\n",
    "            public_url = None\n",
    "\n",
    "        # Run the app in a separate thread\n",
    "        thread = threading.Thread(target=run_app)\n",
    "        thread.start()\n",
    "\n",
    "        if public_url:\n",
    "            print(\"Your Chainlit app is now running. Access it using the Public URL provided above.\")\n",
    "        else:\n",
    "            print(\"Your Chainlit app is now running. Use Colab's 'Public URL' feature to access it.\")\n",
    "            print(\"Go to Runtime > Manage Sessions > Public URL to get the URL.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error starting app: {e}\")\n",
    "\n",
    "def stop_app():\n",
    "    global public_url, thread, process, stop_flag\n",
    "\n",
    "    stop_flag.set()\n",
    "\n",
    "    # Stop the Chainlit process\n",
    "    if process:\n",
    "        try:\n",
    "            os.kill(process.pid, signal.SIGTERM)\n",
    "        except ProcessLookupError:\n",
    "            pass\n",
    "        process = None\n",
    "\n",
    "    # Stop the thread\n",
    "    if thread:\n",
    "        thread.join(timeout=5)\n",
    "        thread = None\n",
    "\n",
    "    # Close the ngrok tunnel\n",
    "    if public_url:\n",
    "        try:\n",
    "            ngrok.disconnect(public_url)\n",
    "        except Exception as e:\n",
    "            print(f\"Error disconnecting ngrok: {e}\")\n",
    "        public_url = None\n",
    "\n",
    "    print_logs()\n",
    "    output_buffer.truncate(0)\n",
    "    output_buffer.seek(0)\n",
    "    error_buffer.truncate(0)\n",
    "    error_buffer.seek(0)\n",
    "\n",
    "    print(\"Chainlit app has been stopped and ngrok tunnel closed.\")\n",
    "\n",
    "# Start the app\n",
    "start_app()\n",
    "\n",
    "# Keep the Colab notebook running\n",
    "IPython.display.Javascript(\"\"\"\n",
    "    function ClickConnect(){\n",
    "        console.log(\"Working\");\n",
    "        document.querySelector(\"colab-connect-button\").click()\n",
    "    }\n",
    "    setInterval(ClickConnect, 60000)\n",
    "\"\"\")\n",
    "\n",
    "# To stop the app, run the following function:\n",
    "# stop_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import signal\n",
    "from pyngrok import ngrok\n",
    "\n",
    "def stop_running_app():\n",
    "    # Find and stop the Chainlit process\n",
    "    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "        try:\n",
    "            # Check if the process is the Chainlit app\n",
    "            if 'chainlit' in proc.info['name'] or any('chainlit' in arg for arg in proc.info['cmdline']):\n",
    "                print(f\"Stopping Chainlit process (PID: {proc.info['pid']})\")\n",
    "                os.kill(proc.info['pid'], signal.SIGTERM)\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
    "            pass\n",
    "\n",
    "    # Close all ngrok tunnels\n",
    "    try:\n",
    "        tunnels = ngrok.get_tunnels()\n",
    "        for tunnel in tunnels:\n",
    "            print(f\"Closing ngrok tunnel: {tunnel.public_url}\")\n",
    "            ngrok.disconnect(tunnel.public_url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error closing ngrok tunnels: {e}\")\n",
    "\n",
    "    print(\"Attempted to stop the Chainlit app and close ngrok tunnels.\")\n",
    "\n",
    "# Run the function to stop the app\n",
    "stop_running_app()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
