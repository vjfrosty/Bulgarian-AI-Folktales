{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "import os\n",
    "os.chdir(r'C:\\Users\\a1bg532573\\repo\\Bulgarian-AI-Folktales\\preprocesing')\n",
    "# imports \n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader  # Importing PDF loader from Langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Importing text splitter from Langchain\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings # Importing OpenAI embeddings from Langchain\n",
    "from langchain.schema import Document  # Importing Document schema from Langchain\n",
    "from langchain_chroma import Chroma  # Importing Chroma vector store from Langchain\n",
    "from dotenv import load_dotenv # Importing dotenv to get API key from .env file\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import os  # Importing os module for operating system functionalities\n",
    "import shutil  # Importing shutil module for high-level file operations\n",
    "\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load documents and clean duplicates`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(DATA_PATH):\n",
    "    \"\"\"\n",
    "    Load text documents from JSON files in the specified directory.\n",
    "\n",
    "    Returns:\n",
    "        List of Document objects: Loaded text documents represented as Langchain Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in os.listdir(DATA_PATH):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(DATA_PATH, filename)\n",
    "            \n",
    "            # Open and read each JSON file\n",
    "            with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                \n",
    "                # Extract book, author, and stories from JSON\n",
    "                book_name = data.get(\"book\", \"\")\n",
    "                author = data.get(\"author\", \"\")\n",
    "                stories = data.get(\"stories\", [])\n",
    "\n",
    "                # Iterate through each story and create Document objects\n",
    "                for story in stories:\n",
    "                    if \"story\" in story or \"author\" in story:\n",
    "                        story_title = story.get(\"story\") or story.get(\"author\")\n",
    "                        story_content = story.get(\"content\", \"\")\n",
    "\n",
    "                        # Append the story title to the beginning of the story content\n",
    "                        combined_content = f\"{story_title}: {story_content}\"\n",
    "\n",
    "                        # Create a Document object with metadata\n",
    "                        document = Document(\n",
    "                            page_content=combined_content,\n",
    "                            metadata={\n",
    "                                \"book\": book_name,\n",
    "                                \"author\": author,\n",
    "                                \"story\": story_title\n",
    "                            }\n",
    "                        )\n",
    "                        # Append the document to the list of documents\n",
    "                        documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def keep_unique_documents(documents):\n",
    "    \"\"\"\n",
    "    Keep only one unique record for each duplicate, removing redundant records.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of Document objects.\n",
    "\n",
    "    Returns:\n",
    "        List of Document objects: List with unique documents, retaining one instance for each duplicate.\n",
    "    \"\"\"\n",
    "    content_tracker = {}\n",
    "    unique_documents = []\n",
    "\n",
    "    # Track each document's combined content and keep only the first occurrence\n",
    "    for doc in documents:\n",
    "        content = doc.page_content.strip().lower()  # Normalize for comparison\n",
    "        if content not in content_tracker:\n",
    "            # Add the first occurrence to the unique list\n",
    "            content_tracker[content] = doc\n",
    "            unique_documents.append(doc)\n",
    "\n",
    "    return unique_documents\n",
    "\n",
    "DATA_PATH = 'output_json_files'\n",
    "documents = load_documents(DATA_PATH)  # Load documents from a source\n",
    "unique_documents = keep_unique_documents(documents)  # Keep only unique documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building a small quick token size function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"\n",
    "    Count the number of tokens in a given text using the specified model's tokenizer.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to tokenize.\n",
    "        model (str): The name of the model to use for tokenization (default: \"gpt-3.5-turbo\").\n",
    "\n",
    "    Returns:\n",
    "        int: The number of tokens in the text.\n",
    "    \"\"\"\n",
    "    model = \"gpt-4o-mini\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sort documents by token size and print first and last examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: повест за една гора... | Tokens: 75356\n",
      "Document: срещата на най-големия с ламят... | Tokens: 18720\n",
      "Document: щетинското ханче... | Tokens: 10889\n",
      "Document: в тронната зала... | Tokens: 8410\n",
      "Document: веселият монах... | Tokens: 7511\n",
      "Document: босата команда... | Tokens: 6840\n",
      "Document: гарван грачи... | Tokens: 5535\n",
      "Document: юнакът със звезда на челото и ... | Tokens: 5135\n",
      "Document: юнакът със звезда на челото и ... | Tokens: 5129\n",
      "Document: врабчетата на стрина дойна... | Tokens: 4829\n",
      "---------------\n",
      "Document: жените или мъжете са повече на земята?: където ходели настрадин ходжа и хитър петър, все се нещо препирали, изпитвали, надлъгвали. като вървели веднъж. хитър петър разправял, че жените са повече на земното кълбо, а ходжата казвал, че мъжете са повече. слушал го, слушал хитър петър, па му рекъл — не си прав, ходжа! жените са повече, защото има мъже, които слушат жените си затова и те се числят към жените!... | Tokens: 141\n",
      "Document: изгубил доверие: в селото на хитър петър дошел другоселец да си купува вол. хитър петър го водел от къща на къща да изберат млад и хубав вол. като вървели из улицата хитър петър рекъл на другоселеца — байо, дай ми сто гроша назаем! — ами аз те не познавам! — рекъл му другоселецът. — та аз тъкмо затова искам от тебе — рекъл хитър петър, — че ония, които ме познават, не ми дават... | Tokens: 140\n",
      "Document: колко са завоите в целия свят: едни шегобийци срещнали хитър петър и като искали да се подиграят с него, рекли — хайде, петре, като си толкова хитър, можеш ли ни каза — колко завои има из целия свят? хитър петър отговорил на часа — от това по-лесно няма! завоите са само два — ляв и десен! шегобийците останали с пръст в устата, че не могли и тоя път да надхитрят хитър петър.... | Tokens: 134\n",
      "Document: залъкът на богаташа: един богаташин се карал нещо на сиромасите и почнал да им вика — аз съм си купил голям имот, ама съм си отделял от залъка! а вие всичко изяждате, затова сте фукари! зачул го хитър петър и му казва — ега ти залъкът! какъв ли ще е тоя толкоз голям залък, та да спечелиш такова голямо богатство и да купиш такъв имот!... | Tokens: 125\n",
      "Document: срещу нова година: спи, моя палава сестричке, спят зайчета, щурци и птички. след малко тихо през комина ще дойде новата година. и кой каквото си сънува наяве тя ще му дарува на катеричката — бадеми, на зайо — моркови големи, на тебе — кукла за другарче, на мене — шарено букварче. спи, някой идва през комина. дали е новата година?... | Tokens: 124\n",
      "Document: книжки мои: писмо тихо затваряйте в къщи вратите! тихо пристъпвайте — да не шумите! катето нека остане само, катето пише днес първо писмо. катето всяка стотинка пестило, та да си купи и плик, и мастило, та да напише с трептяща ръчичка бабо, и аз съм сега ученичка. скоро голямо писмо ще ти пратя. поздрав от всички. целува те катя.... | Tokens: 123\n",
      "Document: млекари: дълго време хитър петър и разумни радой продавали мляко. радой виждал, че хитър петър продавал много повече мляко от него, та веднъж го запитал — петре, ти колко крави доиш? — две крави. — и колко мляко надояваш от тях? — надоявам пет-шест кила. — а колко продаваш всеки ден? — както се случи понякога петнайсет, понякога двайсет... | Tokens: 122\n",
      "Document: хитрият гайдарджия: синът на хитър петър се учел да свири на гайда. баща му, за да го насърчи да свири, му рекъл — сине, като свириш от сутрин до вечер, ще ти давам всеки ден по едно петаче! а малкият хитрец отговорил — нямам сметка, тате, защото съседите ми дават по грош на ден, за да не свиря.... | Tokens: 109\n",
      "Document: тънка сметка: хитър петър го съдили. кадията му казал — осъждам те на двайсет и пет дена затвор! — мога ли, кади ефенди, да ги излежа през зимата? — попитал хитър петър. — защо пък искаш през зимата? — запитал го кадията. — защото тогава дните са по-къси — отговорил хитър петър.... | Tokens: 105\n",
      "Document: защо е несресан волът?: веднъж хитър петър карал два вола из улицата. срещнали го някои шегаджии и го запитали — петре, защо ти е единият вол рошав? ти не си го гледал добре! хитър петър отговорил — тоя вол не се е бръснал, че е умрял баща му. затова ходи рошав.... | Tokens: 101\n",
      "Document: нощ: темно, брате, темно, ете, като в катраница, нито месечинка свети, ни ясна звездица! куче лае у полето, та си глава кине — ергени ли мома краднат, или некой гине? та нема ли кой да палне плевнико на кмето да разпусне това пусто темнило проклето!... | Tokens: 96\n",
      "Document: как големците излизат на пазар: един ден по тържището минавал някой си големец, а подир него вървял неговият слуга. като го съгледал хитър петър казал на хората — тоя човек се е научил да ходи като магарето, което не върви, додето не го карат отподире му... | Tokens: 87\n",
      "Document: на мама: задето песни си ми пяла, край мене ден и нощ си бдяла и ме научи да се трудя и рано-рано да се будя, а в нашата родина свята да пазя вярно свободата — три думи ще ти кажа само — благодаря, обична мамо!... | Tokens: 72\n",
      "Document: ран босилек: неродена мома. незнаен юнак. жива вода неродена мома. незнаен юнак. жива вода - народни приказки неродена мома. незнаен юнак. жива вода - народни приказки ран босилек... | Tokens: 64\n",
      "Document: елин пелин: съчинения в шест тома — том пети елин пелин съчинения в шест тома — том пети... | Tokens: 33\n",
      "Document: леда милева: в една зоологическа градина леда милева в една зоологическа градина леда милева... | Tokens: 31\n",
      "Document: елин пелин: сватбата на червенушко елин пелин весела приказка в стихове... | Tokens: 27\n",
      "Document: сава попов: хитър петър народни приказки - хитър петър хумор сава попов... | Tokens: 25\n",
      "Document: горска хубавица: народни приказки горска хубавица народни приказки народни приказки... | Tokens: 23\n",
      "Document: ангел каралийчев: български народни приказки - том 2... | Tokens: 18\n",
      "Document: ангел каралийчев: български народни приказки том 1... | Tokens: 17\n",
      "Document: ангел каралийчев: имало едно време - народни приказки... | Tokens: 17\n",
      "Document: ангел каралийчев: тошко африкански и приказки... | Tokens: 17\n",
      "Document: ангел каралийчев: български народни приказки ак2... | Tokens: 16\n",
      "Document: ангел каралийчев: лъв без опашка... | Tokens: 13\n",
      "Document: ангел каралийчев: приказки и разкази... | Tokens: 13\n",
      "Document: през планини и морета: момчето без име... | Tokens: 13\n",
      "Document: елин пелин: пижо и пендо... | Tokens: 12\n",
      "Document: елин пелин: повести и разкази... | Tokens: 12\n",
      "Document: елин пелин: гори тилилейски... | Tokens: 11\n"
     ]
    }
   ],
   "source": [
    "# sort unique_documents by unique_documents[i].page_content token size\n",
    "sorted_documents = sorted(unique_documents, key=lambda x: count_tokens(x.page_content), reverse=True)\n",
    "\n",
    "# Print the first 10 documents\n",
    "for doc in sorted_documents[:10]:\n",
    "    print(f\"Document: {doc.metadata['story'][:30]}... | Tokens: {count_tokens(doc.page_content)}\")\n",
    "print(\"---------------\")\n",
    "# Print last 10 documents\n",
    "for doc in sorted_documents[-30:]:\n",
    "    print(f\"Document: {doc.page_content}... | Tokens: {count_tokens(doc.page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sort docments in bins based on token counts\n",
    "#### 1. Dropping binned_documents['0-60'] as it is too small and possible left noise from parsing\n",
    "#### 2. Taking bin ['60-1000'] as full ebedding\n",
    "#### 3. Taking bin ['1000-'float('inf')] as chunk embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin 0-60: 16 documents\n",
      "Bin 60-1000: 398 documents\n",
      "Bin 1000-inf: 433 documents\n",
      "\n",
      "Bin 0-60:\n",
      "  Document: елин пелин... | Tokens: 33\n",
      "  Document: леда милева... | Tokens: 31\n",
      "  Document: елин пелин... | Tokens: 27\n",
      "  Document: сава попов... | Tokens: 25\n",
      "  Document: горска хубавица... | Tokens: 23\n",
      "  Document: ангел каралийчев... | Tokens: 18\n",
      "  Document: ангел каралийчев... | Tokens: 17\n",
      "  Document: ангел каралийчев... | Tokens: 17\n",
      "  Document: ангел каралийчев... | Tokens: 17\n",
      "  Document: ангел каралийчев... | Tokens: 16\n",
      "  Document: ангел каралийчев... | Tokens: 13\n",
      "  Document: ангел каралийчев... | Tokens: 13\n",
      "  Document: през планини и морета... | Tokens: 13\n",
      "  Document: елин пелин... | Tokens: 12\n",
      "  Document: елин пелин... | Tokens: 12\n",
      "  Document: елин пелин... | Tokens: 11\n",
      "  Last 20 documents in 0-60:\n",
      "    Document: елин пелин... | Tokens: 33\n",
      "    Document: леда милева... | Tokens: 31\n",
      "    Document: елин пелин... | Tokens: 27\n",
      "    Document: сава попов... | Tokens: 25\n",
      "    Document: горска хубавица... | Tokens: 23\n",
      "    Document: ангел каралийчев... | Tokens: 18\n",
      "    Document: ангел каралийчев... | Tokens: 17\n",
      "    Document: ангел каралийчев... | Tokens: 17\n",
      "    Document: ангел каралийчев... | Tokens: 17\n",
      "    Document: ангел каралийчев... | Tokens: 16\n",
      "    Document: ангел каралийчев... | Tokens: 13\n",
      "    Document: ангел каралийчев... | Tokens: 13\n",
      "    Document: през планини и морета... | Tokens: 13\n",
      "    Document: елин пелин... | Tokens: 12\n",
      "    Document: елин пелин... | Tokens: 12\n",
      "    Document: елин пелин... | Tokens: 11\n",
      "\n",
      "Bin 60-1000:\n",
      "  Document: трите патенца... | Tokens: 999\n",
      "  Document: сън... | Tokens: 993\n",
      "  Document: клан-недоклан... | Tokens: 978\n",
      "  Document: как хитър петър засрамил своя ... | Tokens: 975\n",
      "  Document: хитър петър лъже, настрадин хо... | Tokens: 963\n",
      "  Document: оскубаната гъска... | Tokens: 955\n",
      "  Document: приказка за патката и пушката... | Tokens: 952\n",
      "  Document: косе босе... | Tokens: 942\n",
      "  Document: наддумване... | Tokens: 942\n",
      "  Document: магарето на сюлейман ага... | Tokens: 939\n",
      "  Document: излъганите лъжци... | Tokens: 936\n",
      "  Document: гнездо в гнездо... | Tokens: 934\n",
      "  Document: подплашените вълци... | Tokens: 932\n",
      "  Document: мечтите на едно момче... | Tokens: 929\n",
      "  Document: цар цървулан... | Tokens: 926\n",
      "  Document: чисто носи — сладко яде... | Tokens: 926\n",
      "  Document: котето и лъвът... | Tokens: 925\n",
      "  Document: вълк и козел... | Tokens: 924\n",
      "  Document: ученикът надминал учителя си... | Tokens: 921\n",
      "  Document: хитър петър оправя турското ца... | Tokens: 914\n",
      "  ...\n",
      "  Last 20 documents in 60-1000:\n",
      "    Document: иван жеглов... | Tokens: 152\n",
      "    Document: случка... | Tokens: 151\n",
      "    Document: като виждаш, не питай!... | Tokens: 146\n",
      "    Document: свири на цигулка... | Tokens: 144\n",
      "    Document: десет гроша... | Tokens: 143\n",
      "    Document: не е хигиенично... | Tokens: 142\n",
      "    Document: жените или мъжете са повече на... | Tokens: 141\n",
      "    Document: изгубил доверие... | Tokens: 140\n",
      "    Document: колко са завоите в целия свят... | Tokens: 134\n",
      "    Document: залъкът на богаташа... | Tokens: 125\n",
      "    Document: срещу нова година... | Tokens: 124\n",
      "    Document: книжки мои... | Tokens: 123\n",
      "    Document: млекари... | Tokens: 122\n",
      "    Document: хитрият гайдарджия... | Tokens: 109\n",
      "    Document: тънка сметка... | Tokens: 105\n",
      "    Document: защо е несресан волът?... | Tokens: 101\n",
      "    Document: нощ... | Tokens: 96\n",
      "    Document: как големците излизат на пазар... | Tokens: 87\n",
      "    Document: на мама... | Tokens: 72\n",
      "    Document: ран босилек... | Tokens: 64\n",
      "\n",
      "Bin 1000-inf:\n",
      "  Document: повест за една гора... | Tokens: 75356\n",
      "  Document: срещата на най-големия с ламят... | Tokens: 18720\n",
      "  Document: щетинското ханче... | Tokens: 10889\n",
      "  Document: в тронната зала... | Tokens: 8410\n",
      "  Document: веселият монах... | Tokens: 7511\n",
      "  Document: босата команда... | Tokens: 6840\n",
      "  Document: гарван грачи... | Tokens: 5535\n",
      "  Document: юнакът със звезда на челото и ... | Tokens: 5135\n",
      "  Document: юнакът със звезда на челото и ... | Tokens: 5129\n",
      "  Document: врабчетата на стрина дойна... | Tokens: 4829\n",
      "  Document: в изгнание... | Tokens: 4780\n",
      "  Document: най-хубавата земя... | Tokens: 4680\n",
      "  Document: колко народ се беше насъбрал д... | Tokens: 4621\n",
      "  Document: братче и сестриче... | Tokens: 4344\n",
      "  Document: сиромашка правда... | Tokens: 4310\n",
      "  Document: момчето, кученцето, котенцето ... | Tokens: 4286\n",
      "  Document: хитрецът и царската дъщеря... | Tokens: 4217\n",
      "  Document: отец герасим... | Tokens: 4154\n",
      "  Document: симовци... | Tokens: 4131\n",
      "  Document: бялото гълъбче... | Tokens: 4079\n",
      "  ...\n",
      "  Last 20 documents in 1000-inf:\n",
      "    Document: хитър петър надхитря настрадин... | Tokens: 1105\n",
      "    Document: по следите на ламята... | Tokens: 1104\n",
      "    Document: мечката и мравките... | Tokens: 1095\n",
      "    Document: лъв и човек... | Tokens: 1085\n",
      "    Document: котаран... | Tokens: 1083\n",
      "    Document: хитрините на кума-лиса... | Tokens: 1078\n",
      "    Document: умна девойка... | Tokens: 1072\n",
      "    Document: старият елен и малкото еленче... | Tokens: 1068\n",
      "    Document: как се запознали хитър петър и... | Tokens: 1067\n",
      "    Document: човекът и лъвът... | Tokens: 1064\n",
      "    Document: ослепената кокошка... | Tokens: 1059\n",
      "    Document: лъв и човек... | Tokens: 1059\n",
      "    Document: гостолюбивите мравки... | Tokens: 1057\n",
      "    Document: гудо и агудо... | Tokens: 1055\n",
      "    Document: как хитър петър получил името ... | Tokens: 1046\n",
      "    Document: вълкът си е вълк... | Tokens: 1041\n",
      "    Document: дядовата ръкавичка... | Tokens: 1036\n",
      "    Document: кон и лисица... | Tokens: 1028\n",
      "    Document: умни хора... | Tokens: 1016\n",
      "    Document: врабчето си иска зърното... | Tokens: 1009\n"
     ]
    }
   ],
   "source": [
    "def sort_documents_into_bins(documents, bin_ranges):\n",
    "    \"\"\"\n",
    "    Sort documents into bins based on their token counts.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of Document objects.\n",
    "        bin_ranges (list): List of tuples representing the start and end of each bin range.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with bin ranges as keys and sorted lists of documents as values.\n",
    "    \"\"\"\n",
    "    bins = {f\"{start}-{end}\": [] for start, end in bin_ranges}\n",
    "    \n",
    "    for doc in documents:\n",
    "        token_count = count_tokens(doc.page_content)\n",
    "        for start, end in bin_ranges:\n",
    "            if start <= token_count < end:\n",
    "                bins[f\"{start}-{end}\"].append((doc, token_count))\n",
    "                break\n",
    "    \n",
    "    # Sort documents within each bin by token count (descending order)\n",
    "    for bin_range in bins:\n",
    "        bins[bin_range].sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return bins\n",
    "\n",
    "# Define bin ranges\n",
    "bin_ranges = [(0, 60), (60, 1000), (1000, float('inf'))]\n",
    "\n",
    "# Sort documents into bins\n",
    "binned_documents = sort_documents_into_bins(unique_documents, bin_ranges)\n",
    "\n",
    "# Print the number of documents in each bin\n",
    "for bin_range, docs in binned_documents.items():\n",
    "    print(f\"Bin {bin_range}: {len(docs)} documents\")\n",
    "\n",
    "# Print example documents from each bin\n",
    "for bin_range, docs in binned_documents.items():\n",
    "    print(f\"\\nBin {bin_range}:\")\n",
    "    for doc, token_count in docs[:20]:  # Print first 20 documents in each bin\n",
    "        print(f\"  Document: {doc.metadata['story'][:30]}... | Tokens: {token_count}\")\n",
    "    if len(docs) > 20:\n",
    "        print(\"  ...\")\n",
    "    # Print last 20 documents in each bin\n",
    "    print(f\"  Last 20 documents in {bin_range}:\")\n",
    "    for doc, token_count in docs[-20:]:\n",
    "        print(f\"    Document: {doc.metadata['story'][:30]}... | Tokens: {token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Custom Contextual Enhancing embedding for ['60-1000'] bin (**Small Stories**) and ['1000-inf'] bin (**Large Stories**) with GPT-4o-mini\n",
    "\n",
    "- Timeout mechanism to avoid hitting TPM limit of gpt-4o-mini (200 000 TPM) - keep track of the number of tokens sent to the LLM and pause for 1 minute the process if the limit is about to be reached\n",
    "- Handling large documents:\n",
    "1. Document size: Handles texts ranging from 1,000 to 20,000 tokens.\n",
    "2. Dynamic chunking: Uses a text splitter to divide documents into ~1,000 token chunks with 200 token overlap. Chunk count adapts to document length.\n",
    "3. Contextual embedding: Applies `get_contextual_embedding()` with `prompt_type=\"contextual\"` to each chunk, providing context within the full document.\n",
    "4. Document-level enhancement: Calls `get_contextual_embedding()` with `prompt_type=\"enhancing\"` once for the entire document.\n",
    "5. Information combination: Appends both chunk-specific contextual info and document-level enhancing info to each chunk, resulting in multiple chunks with shared enhancing info but unique contextual details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded small document: трите патенца...\n",
      "Total tokens: 999\n",
      "Embedded large document: врабчето си иска зърното...\n",
      "Number of splits: 2 for Character count: 2523 for Tokens 1009\n",
      "Total tokens: 2008\n",
      "Saved 3 documents to vector_store_test.\n",
      "Finished embedding all documents and saved to Chroma DB.\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "TEST_CHROMA_PATH = \"vector_store_test\"\n",
    "TPM_LIMIT = 200000\n",
    "PAUSE_TIME = 80  # seconds\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize components\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "# Set up ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",  # Use a smaller model for faster response times\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0,       # Increase temperature for more creativity\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "def get_contextual_embedding(doc: Document, llm: ChatOpenAI, prompt_type: str, full_text: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Generate a contextual embedding for a document using the LLM.\n",
    "    \n",
    "    Args:\n",
    "        doc (Document): The document or chunk to embed.\n",
    "        llm (ChatOpenAI): The language model to use for generating context.\n",
    "        prompt_type (str): Type of prompt to use (\"contextual\" or \"enhancing\").\n",
    "        full_text (str): The full text of the document (used for contextual embedding).\n",
    "    \n",
    "    Returns:\n",
    "        str: The contextual embedding as a string.\n",
    "    \"\"\"\n",
    "    if prompt_type == \"contextual\":\n",
    "        prompt = f\"\"\"\n",
    "        Here is the chunk we want to situate within the whole document\n",
    "        <chunk>\n",
    "        {doc.page_content}\n",
    "        </chunk>\n",
    "\n",
    "        Here is the content of the whole document\n",
    "        <document>\n",
    "        {full_text}\n",
    "        </document>\n",
    "\n",
    "        Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "        Answer only with the succinct context and nothing else.\n",
    "        \"\"\"\n",
    "    elif prompt_type == \"enhancing\":\n",
    "        prompt = f\"\"\"\n",
    "        Based on the following story, provide a concise summary including:\n",
    "        1. A brief synopsis of the story\n",
    "        2. The main characters\n",
    "        3. The setting or environment\n",
    "        4. The moral or main message of the story\n",
    "        <story>\n",
    "        Story: {doc.page_content}\n",
    "        </story>\n",
    "\n",
    "        Respond in a concise paragraph format: \"**brief summary of the story:** ..., **main characters:** ..., **setting:** ..., **moral:** ...\" *... are placeholders for the actual values you will provide after reading the story. Please give a succinct answer to augment the document for the purposes of improving search retrieval of the story.\n",
    "        Separate every value in a separate line with a new line character. \n",
    "        \"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response.content\n",
    "\n",
    "def embed_documents(documents: List[Tuple[Document, int]], llm: ChatOpenAI, embeddings, chroma_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Embed documents with contextual information and store in Chroma DB.\n",
    "    Handles both small (60-1000 tokens) and large (1000+ tokens) documents differently.\n",
    "    \n",
    "    Args:\n",
    "        documents (List[Tuple[Document, int]]): List of (document, token_count) tuples.\n",
    "        llm (ChatOpenAI): The language model to use for generating context.\n",
    "        embeddings: The embedding model to use.\n",
    "        chroma_path (str): The path to store the Chroma database.\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    start_time = time.time()\n",
    "    combined_docs = []\n",
    "\n",
    "    for doc, token_count in documents:\n",
    "        # Check if we're approaching the TPM limit\n",
    "        if total_tokens + token_count > TPM_LIMIT:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            if elapsed_time < PAUSE_TIME:\n",
    "                print(f\"Approaching TPM limit. Pausing for {PAUSE_TIME - elapsed_time:.2f} seconds.\")\n",
    "                time.sleep(PAUSE_TIME - elapsed_time)\n",
    "            total_tokens = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "        # Determine if it's a large document (1000+ tokens)\n",
    "        is_large_doc = token_count >= 1000\n",
    "\n",
    "        if is_large_doc:\n",
    "            # Calculate the number of chunks based on character count\n",
    "            character_count = len(doc.page_content)\n",
    "            num_chunks = max(2, token_count // 1000)\n",
    "            chunk_size = (character_count // num_chunks) + 200\n",
    "            \n",
    "            # Create a text splitter\n",
    "            text_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=200,\n",
    "                length_function=len,\n",
    "            )\n",
    "\n",
    "            # Split the document into chunks\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "            # Generate enhancing embedding for the whole document\n",
    "            enhancing_info = get_contextual_embedding(doc, llm, prompt_type=\"enhancing\")\n",
    "\n",
    "            # Process each chunk\n",
    "            for chunk in chunks:\n",
    "                # Generate contextual embedding for the chunk\n",
    "                contextual_info = get_contextual_embedding(chunk, llm, prompt_type=\"contextual\", full_text=doc.page_content)\n",
    "\n",
    "                # Combine original content with contextual and enhancing info\n",
    "                combined_content = f\"{chunk.page_content}\\n\\nContextual Information:\\n{contextual_info}\\n\\nEnhancing Information:\\n{enhancing_info}\"\n",
    "                \n",
    "                # Create a new document with combined content and original metadata\n",
    "                contextual_doc = Document(page_content=combined_content, metadata=doc.metadata)\n",
    "                \n",
    "                combined_docs.append(contextual_doc)\n",
    "\n",
    "            print(f\"Embedded large document: {doc.metadata['story'][:30]}...\")\n",
    "            print(f\"Number of splits: {len(chunks)} for Character count: {character_count} for Tokens {token_count}\")\n",
    "        else:\n",
    "            # For smaller documents, use the original approach\n",
    "            enhancing_info = get_contextual_embedding(doc, llm, prompt_type=\"enhancing\")\n",
    "            \n",
    "            # Combine original content with enhancing info\n",
    "            combined_content = f\"{doc.page_content}\\n\\nEnhancing Information:\\n{enhancing_info}\"\n",
    "            \n",
    "            # Create a new document with combined content and original metadata\n",
    "            contextual_doc = Document(page_content=combined_content, metadata=doc.metadata)\n",
    "            \n",
    "            combined_docs.append(contextual_doc)\n",
    "            print(f\"Embedded small document: {doc.metadata['story'][:30]}...\")\n",
    "\n",
    "        total_tokens += token_count\n",
    "        print(f\"Total tokens: {total_tokens}\")\n",
    "    \n",
    "    # Create and persist the Chroma database\n",
    "    db = Chroma.from_documents(\n",
    "        combined_docs, embeddings, persist_directory=chroma_path\n",
    "    )\n",
    "    print(f\"Saved {len(combined_docs)} documents to {chroma_path}.\")\n",
    "\n",
    "# Test with documents from both bins\n",
    "small_docs = binned_documents['60-1000'][:1]\n",
    "large_docs = [binned_documents['1000-inf'][-1]]\n",
    "test_docs = small_docs + large_docs\n",
    "\n",
    "embed_documents(test_docs, llm, embeddings, TEST_CHROMA_PATH)\n",
    "print(\"Finished embedding all documents and saved to Chroma DB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Testing the retrieval of the stored embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['b51a5ca9-5852-47dc-8a76-518726a93bc1',\n",
       "  '87a8cc1b-58d6-4146-a101-a59db6e670ed',\n",
       "  '0f97b8f7-7fb5-4eb4-a809-5922889cdd8c'],\n",
       " 'embeddings': array([[ 0.06076654,  0.02530093,  0.01129333, ...,  0.0064741 ,\n",
       "         -0.01222809,  0.03312524],\n",
       "        [ 0.03261885,  0.0120194 , -0.03974689, ...,  0.00992206,\n",
       "          0.02024011,  0.04889894],\n",
       "        [ 0.01968575,  0.01906617, -0.04697545, ...,  0.02292447,\n",
       "          0.00985696,  0.05542427]]),\n",
       " 'metadatas': None,\n",
       " 'documents': None,\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['embeddings']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = Chroma(persist_directory=TEST_CHROMA_PATH)\n",
    "result = db.get(include=['embeddings'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['87a8cc1b-58d6-4146-a101-a59db6e670ed'],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [{'author': 'ангел каралийчев',\n",
       "   'book': 'Български народни приказки ак',\n",
       "   'story': 'врабчето си иска зърното'}],\n",
       " 'documents': [\"врабчето си иска зърното: сивото врабче кацнало върху един плет и почнало да си ниже герданче от мънистени зрънца. както нижело, изтървало едно зърно. зърното паднало в тръните, търколило се някъде и се загубило. — хей, плет — изчуруликало врабчето, — дай ми мънистеното зърно или ще кажа на огъня да те изгори! — кажи му де! — отвърнал плетът. — огънчо — хвръкнало врабчето над огъня, — изгори плета! — не ща — отвърнал огънят. — докато си имам сухи букови дървета, много ми е притрябвало да горя трънливия плет и да се бода на тръните му. — ще кажа на реката да те угаси! — кажи й де! — отвърнал огънят. врабчето литнало над реката и зачуруликало. — речице, моля ти се, угаси огъня! — ами — отговорила реката, — много ми е притрябвало да гася огън. додето си имам тия гладки камъчета, които сега броя, що ми трябва да се паря с огън? — ще кажа на бивола да те изпие! — заканило се врабчето. — кажи му де! — биволчо, изпий реката! — кацнало врабчето върху единия рог на бивола. — как не — отвърнал биволът, — аз се напасох с такава росна трева, че ако сръбна и вода — ще ми се надуе коремът и ще се пукне. — тогава ще кажа на вълка да те изяде. — кажи му де! — рекъл биволът. врабчето отишло при вълка в гората. — вълчо — помолило го то, — ела да изядеш бивола. — какво приказваш — отговорил вълкът, — додето има такива крехки агънца, що ми трябва жилаво биволско месо! — ще кажа на овчаря да насъска кучетата и те ще ти разкъсат кожуха! — кажи му де! —\\n\\nContextual Information:\\nThe chunk is a narrative segment from a fable about a sparrow trying to retrieve a lost bead from a thorny hedge. It details the sparrow's attempts to threaten various elements of nature (fire, river, buffalo, wolf, shepherd) to get the bead back, showcasing a chain of interactions among these characters. This section illustrates themes of resourcefulness and the interconnectedness of nature, ultimately leading to the resolution where the sparrow receives the bead.\\n\\nEnhancing Information:\\n**brief summary of the story:** A gray sparrow loses a bead while stringing them on a fence and demands it back, threatening various elements of nature. Each entity refuses to comply, leading to a chain reaction of threats until a cat intervenes, causing a series of events that ultimately results in the sparrow retrieving its lost bead from the fence. \\n\\n**main characters:** The gray sparrow, the fence, the fire, the river, the buffalo, the wolf, the shepherd, the mice, and the cat.\\n\\n**setting:** The story takes place in a natural environment featuring a fence, a fire, a river, and a forest.\\n\\n**moral:** The story illustrates the futility of threats and the interconnectedness of nature, emphasizing that cooperation and understanding are more effective than intimidation.\"],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['metadatas', 'documents']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.get('87a8cc1b-58d6-4146-a101-a59db6e670ed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "врабчето си иска зърното: сивото врабче кацнало върху един плет и почнало да си ниже герданче от мънистени зрънца. както нижело, изтървало едно зърно. зърното паднало в тръните, търколило се някъде и се загубило. — хей, плет — изчуруликало врабчето, — дай ми мънистеното зърно или ще кажа на огъня да те изгори! — кажи му де! — отвърнал плетът. — огънчо — хвръкнало врабчето над огъня, — изгори плета! — не ща — отвърнал огънят. — докато си имам сухи букови дървета, много ми е притрябвало да горя трънливия плет и да се бода на тръните му. — ще кажа на реката да те угаси! — кажи й де! — отвърнал огънят. врабчето литнало над реката и зачуруликало. — речице, моля ти се, угаси огъня! — ами — отговорила реката, — много ми е притрябвало да гася огън. додето си имам тия гладки камъчета, които сега броя, що ми трябва да се паря с огън? — ще кажа на бивола да те изпие! — заканило се врабчето. — кажи му де! — биволчо, изпий реката! — кацнало врабчето върху единия рог на бивола. — как не — отвърнал биволът, — аз се напасох с такава росна трева, че ако сръбна и вода — ще ми се надуе коремът и ще се пукне. — тогава ще кажа на вълка да те изяде. — кажи му де! — рекъл биволът. врабчето отишло при вълка в гората. — вълчо — помолило го то, — ела да изядеш бивола. — какво приказваш — отговорил вълкът, — додето има такива крехки агънца, що ми трябва жилаво биволско месо! — ще кажа на овчаря да насъска кучетата и те ще ти разкъсат кожуха! — кажи му де! —\n",
       "\n",
       "Contextual Information:\n",
       "The chunk is a narrative segment from a fable about a sparrow trying to retrieve a lost bead from a thorny hedge. It details the sparrow's attempts to threaten various elements of nature (fire, river, buffalo, wolf, shepherd) to get the bead back, showcasing a chain of interactions among these characters. This section illustrates themes of resourcefulness and the interconnectedness of nature, ultimately leading to the resolution where the sparrow receives the bead.\n",
       "\n",
       "Enhancing Information:\n",
       "**brief summary of the story:** A gray sparrow loses a bead while stringing them on a fence and demands it back, threatening various elements of nature. Each entity refuses to comply, leading to a chain reaction of threats until a cat intervenes, causing a series of events that ultimately results in the sparrow retrieving its lost bead from the fence. \n",
       "\n",
       "**main characters:** The gray sparrow, the fence, the fire, the river, the buffalo, the wolf, the shepherd, the mice, and the cat.\n",
       "\n",
       "**setting:** The story takes place in a natural environment featuring a fence, a fire, a river, and a forest.\n",
       "\n",
       "**moral:** The story illustrates the futility of threats and the interconnectedness of nature, emphasizing that cooperation and understanding are more effective than intimidation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown(db.get('87a8cc1b-58d6-4146-a101-a59db6e670ed')[\"documents\"][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairy_tale_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
