{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain dependencies\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader  # Importing PDF loader from Langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Importing text splitter from Langchain\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings # Importing OpenAI embeddings from Langchain\n",
    "from langchain.schema import Document  # Importing Document schema from Langchain\n",
    "from langchain_chroma import Chroma  # Importing Chroma vector store from Langchain\n",
    "from dotenv import load_dotenv # Importing dotenv to get API key from .env file\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import os  # Importing os module for operating system functionalities\n",
    "import shutil  # Importing shutil module for high-level file operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-QX2WIwMC3mrFd3nN6PKa1ixf8jbggqFN0_MEa77QscT3BlbkFJKyseiOomj_PaASe5L8gkKJ-tdslbAznXpOVAzQsNoA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable\n",
    "os.environ['OPENAI_API_KEY'] = \n",
    "\n",
    "# Verify that the environment variable is set\n",
    "print(os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"Basic_chroma\"\n",
    "DATA_PATH = \"../preprocesing/output_json_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    "    \n",
    "    # With the `text-embedding-3` class\n",
    "    # of models, you can specify the size\n",
    "    # of the embeddings you want returned.\n",
    "    # dimensions=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'book': 'Български народни приказки ак', 'author': 'ангел каралийчев', 'story': 'ангел каралийчев'} ангел каралийчев: в тази книга са събрани едни от най-добрите приказки от ненадминатия български раз\n",
      "{'book': 'Български народни приказки ак', 'author': 'ангел каралийчев', 'story': 'царят с , магарешките уши'} царят с , магарешките уши: цар траян имал магарешки уши, но ги криел под короната си. никой не знаел\n",
      "{'book': 'Български народни приказки ак', 'author': 'ангел каралийчев', 'story': 'еднооката'} еднооката: имало някога една несвястна жена. мъжът й се трудел от сутрин до вечер, а тя се щурала на\n",
      "{'book': 'Български народни приказки ак', 'author': 'ангел каралийчев', 'story': 'хитрият кръчмар'} хитрият кръчмар: един хитрец отворил кръчма в някакъв град отвъд морето. от всички страни пристигали\n",
      "{'book': 'Български народни приказки ак', 'author': 'ангел каралийчев', 'story': 'сърната и лозата'} сърната и лозата: цял ден ловците преследвали една сърна. надвечер подгоненото животно изскочило от \n",
      "{'book': 'Български народни приказки ак', 'author': 'ангел каралийчев', 'story': 'на лъжата краката са къси'} на лъжата краката са къси: един старец на име делю имал трима синове. те били яки мъже, хубавичко си\n",
      "{'book': 'Български народни приказки ак', 'author': 'ангел каралийчев', 'story': 'овчарят и неговата жена'} овчарят и неговата жена: върху една стара круша паднал гръм и запалил синора. пламнали лозините, къп\n",
      "{'book': 'Български народни приказки ак', 'author': 'ангел каралийчев', 'story': 'мара пепеляшка'} мара пепеляшка: един мъж и една жена имали дъщеря на име мара. бащата ходел да оре по нивите, а майк\n",
      "{'book': 'Български народни приказки ак', 'author': 'ангел каралийчев', 'story': 'кой каквото прави — на себе си го прави'} кой каквото прави — на себе си го прави: като ходел да проси от къща в къща, един слепец все тъй си \n",
      "{'book': 'Български народни приказки ак', 'author': 'ангел каралийчев', 'story': 'вълкът и неговите жертви'} вълкът и неговите жертви: влязла овцата в една кръчма и като обърсала очите си, рекла на кръчмарина \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "def load_documents(DATA_PATH):\n",
    "    \"\"\"\n",
    "    Load text documents from JSON files in the specified directory.\n",
    "\n",
    "    Returns:\n",
    "        List of Document objects: Loaded text documents represented as Langchain Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in os.listdir(DATA_PATH):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(DATA_PATH, filename)\n",
    "            \n",
    "            # Open and read each JSON file\n",
    "            with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                \n",
    "                # Extract book, author, and stories from JSON\n",
    "                book_name = data.get(\"book\", \"\")\n",
    "                author = data.get(\"author\", \"\")\n",
    "                stories = data.get(\"stories\", [])\n",
    "\n",
    "                # Iterate through each story and create Document objects\n",
    "                for story in stories:\n",
    "                    if \"story\" in story or \"author\" in story:\n",
    "                        story_title = story.get(\"story\") or story.get(\"author\")\n",
    "                        story_content = story.get(\"content\", \"\")\n",
    "\n",
    "                        # Append the story title to the beginning of the story content\n",
    "                        combined_content = f\"{story_title}: {story_content}\"\n",
    "\n",
    "                        # Create a Document object with metadata\n",
    "                        document = Document(\n",
    "                            page_content=combined_content,\n",
    "                            metadata={\n",
    "                                \"book\": book_name,\n",
    "                                \"author\": author,\n",
    "                                \"story\": story_title\n",
    "                            }\n",
    "                        )\n",
    "                        # Append the document to the list of documents\n",
    "                        documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "documents = load_documents(DATA_PATH)\n",
    "\n",
    "# Initialize a counter\n",
    "counter = 0\n",
    "\n",
    "# Loop through the documents and print the metadata and first 10 characters of content\n",
    "for doc in documents:\n",
    "    print(doc.metadata, doc.page_content[:100])  # Print metadata and first 10 characters of content\n",
    "    counter += 1\n",
    "    if counter >= 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique documents retained: 847\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain.schema import Document\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_documents(DATA_PATH):\n",
    "    \"\"\"\n",
    "    Load text documents from JSON files in the specified directory.\n",
    "\n",
    "    Returns:\n",
    "        List of Document objects: Loaded text documents represented as Langchain Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in os.listdir(DATA_PATH):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(DATA_PATH, filename)\n",
    "            \n",
    "            # Open and read each JSON file\n",
    "            with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "                data = json.load(json_file)\n",
    "                \n",
    "                # Extract book, author, and stories from JSON\n",
    "                book_name = data.get(\"book\", \"\")\n",
    "                author = data.get(\"author\", \"\")\n",
    "                stories = data.get(\"stories\", [])\n",
    "\n",
    "                # Iterate through each story and create Document objects\n",
    "                for story in stories:\n",
    "                    if \"story\" in story or \"author\" in story:\n",
    "                        story_title = story.get(\"story\") or story.get(\"author\")\n",
    "                        story_content = story.get(\"content\", \"\")\n",
    "\n",
    "                        # Append the story title to the beginning of the story content\n",
    "                        combined_content = f\"{story_title}: {story_content}\"\n",
    "\n",
    "                        # Create a Document object with metadata\n",
    "                        document = Document(\n",
    "                            page_content=combined_content,\n",
    "                            metadata={\n",
    "                                \"book\": book_name,\n",
    "                                \"author\": author,\n",
    "                                \"story\": story_title\n",
    "                            }\n",
    "                        )\n",
    "                        # Append the document to the list of documents\n",
    "                        documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def find_duplicate_documents(documents):\n",
    "    \"\"\"\n",
    "    Find duplicate documents based on combined content.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of Document objects.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary containing duplicated content and corresponding metadata.\n",
    "    \"\"\"\n",
    "    content_tracker = defaultdict(list)\n",
    "\n",
    "    # Track each document's combined content and its metadata\n",
    "    for doc in documents:\n",
    "        content = doc.page_content.strip().lower()  # Normalize for comparison\n",
    "        content_tracker[content].append(doc)\n",
    "\n",
    "    # Find duplicates\n",
    "    duplicates = {content: docs for content, docs in content_tracker.items() if len(docs) > 1}\n",
    "\n",
    "    # Print the duplicated documents\n",
    "    for content, docs in duplicates.items():\n",
    "        print(f\"Duplicated content: '{content[:100]}...' found in the following documents:\")\n",
    "        for doc in docs:\n",
    "            print(f\"  - Book: {doc.metadata['book']}, Author: {doc.metadata['author']}, Story: {doc.metadata['story']}\")\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "def keep_unique_documents(documents):\n",
    "    \"\"\"\n",
    "    Keep only one unique record for each duplicate, removing redundant records.\n",
    "\n",
    "    Args:\n",
    "        documents (list): List of Document objects.\n",
    "\n",
    "    Returns:\n",
    "        List of Document objects: List with unique documents, retaining one instance for each duplicate.\n",
    "    \"\"\"\n",
    "    content_tracker = {}\n",
    "    unique_documents = []\n",
    "\n",
    "    # Track each document's combined content and keep only the first occurrence\n",
    "    for doc in documents:\n",
    "        content = doc.page_content.strip().lower()  # Normalize for comparison\n",
    "        if content not in content_tracker:\n",
    "            # Add the first occurrence to the unique list\n",
    "            content_tracker[content] = doc\n",
    "            unique_documents.append(doc)\n",
    "\n",
    "    return unique_documents\n",
    "\n",
    "# Find and print duplicates\n",
    "#duplicates = find_duplicate_documents(documents)\n",
    "\n",
    "# Keep only unique documents\n",
    "unique_documents = keep_unique_documents(documents)\n",
    "\n",
    "# Print the unique documents count\n",
    "print(f\"\\nTotal unique documents retained: {len(unique_documents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "    \"\"\"\n",
    "    Split the text content of the given list of Document objects into smaller chunks.\n",
    "\n",
    "    Args:\n",
    "        documents (list[Document]): List of Document objects containing text content to split.\n",
    "\n",
    "    Returns:\n",
    "        list[Document]: List of Document objects representing the split text chunks.\n",
    "    \"\"\"\n",
    "    # Initialize text splitter with specified parameters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,  # Size of each chunk in characters\n",
    "        chunk_overlap=200,  # Overlap between consecutive chunks\n",
    "        length_function=len,  # Function to compute the length of the text\n",
    "        add_start_index=True,  # Flag to add start index to each chunk\n",
    "    )\n",
    "    # Split documents into smaller chunks using text splitter\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    # Print example of page content and metadata for a chunk\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks  # Return the list of split text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 847 documents into 5542 chunks.\n",
      "когато мъжът отишъл на нивата, в селото влязъл грънчар с кола грънци и почнал да вика — грънци продавам, грънци! излязла еднооката и попитала — как ги даваш? — пълно за пълно. жената грабнала едно кринче и влязла в житницата да го напълни със зърно. като гребяла, тя напипала заровеното гърне и си рекла — ех, че съм късметлийка! намерих си просото. ще дам на грънчаря просо наместо жито. напълнила кринчето с жълтици и го отнесла. като видял парите, на грънчаря му светнали очите, той поел кринчето, изсипал всичките жълтици в торбата си и побягнал, като оставил и колата, и биволите, и грънците. — брей, че глупав грънчар! — засмяла се еднооката и смъкнала от колата всичките грънци. наредила ги по колците на плета. за всичките грънци имало място, само за едно пукнато гърне нямало. — сторете му\n",
      "{'book': 'Български народни приказки ак', 'author': 'ангел каралийчев', 'story': 'еднооката', 'start_index': 3003}\n",
      "Saved 5542 chunks to Basic_chroma.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "    \n",
    "    # print(chunks)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, embeddings, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    #db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "\n",
    "\n",
    "def generate_data_store():\n",
    "    \"\"\"\n",
    "    Function to generate vector database in chroma from documents.\n",
    "    \"\"\"\n",
    "    documents = load_documents(DATA_PATH)  # Load documents from a source\n",
    "    unique_documents = keep_unique_documents(documents)  # Keep only unique documents\n",
    "    chunks = split_text(unique_documents)  # Split documents into manageable chunks\n",
    "    save_to_chroma(chunks)  # Save the processed data to a data store    \n",
    "\n",
    "# Generate the data store\n",
    "generate_data_store()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
